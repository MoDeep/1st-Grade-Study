# 2018/08/24 정리 : Lec10-1
##sigmoid,ReLU

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-1-1.PNG?raw=true)

n개를 쌓고 싶으면 Weight를 n개 설정해주고 bias도 n개 설정 해준다.
3단이 넘어가면 벡터 값이 복잡해 진다. 
첫번째 W의 [a,b]에서 a만큼 받아들인 후 b만큼 출력하면 두 번째 W는 b만큼 받아들이고 임의의 수 c만큼 내보내면 된다.
bias는 Weight를 구성하는 두 번째값과 같게 만들면 된다.

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-1-2.PNG?raw=true)

그 후 체인연결처럼 쭉 이어주고 최종 값을 가설에 저장해주면 된다.

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-1-3.PNG?raw=true)

위의 것을 실행시키면 아래처럼 나온다.

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-1-4.PNG?raw=true)

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-1-5.PNG?raw=true)

이렇게 나오는 이유는 Backpropagation이라는 이론으로 설명할 수 있다. 

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-1-6.PNG?raw=true)

2단 3단처럼 단이 적을 경우에는 학습이 잘 되는데 단이 많아지면 곱해지는 항이 많아져 최종 미분값이 0에 가깝게 작아진다.
이 말은 값이 별로 영향을 끼치지 못한 다는 뜻이다.(Vanishing gradient)

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-1-7.PNG?raw=true)

이에 대한 해답으로 힌튼 교수는 ReLU라는 함수를 만들었다.

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-1-8.PNG?raw=true)

ReLU : Rectified Linear Unit
값이 0보다 작으면 끄고 크면 사용하는 함수이다.

물론 ReLU를 사용해도 마지막 가설은 sigmoid를 사용한다.
이유는 마지막은 꼭 0과 1사이의 값이 나와야 하기 때문이다.

ReLU와 sigmoid말고도 많은 Activation Functions이 있다.

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-1-9.PNG?raw=true)

