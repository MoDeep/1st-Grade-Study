# 2018/08/24 정리 : Lec10-2
##weight초기화

Vanishing gradient 해결하는 방법 중 1개는 ReLU사용이고, 또 다른 하나는 초기 Weight 값은 잘 설정하는 것이다.

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-2-1.PNG?raw=true)

절대로 0값을 주면 안 된다.

Restricted Boatman Muchine(RBM)을 사용해 초기화를 한 네트워크를 Deep Belief Nets이라고 한다.

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-2-2.PNG?raw=true)

Recreate input
forward 앞의 값을 가지도 임의의 값들을 만들어 낸다. 
Backward 위의 작업을 반대로 한다. 이번엔 뒤에서 앞으로.(Weight을 그대로 사용)

처음에 넣었던 X의 값과 Backward을 한 후 나온 X값을 비교한다.

둘의 차이가 최저가 되도록 Weight를 조절한다.

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-2-3.PNG?raw=true)

두 개씩 잡아서 RBM을 사용해서 학습을 한다.(DBN)

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-2-4.PNG?raw=true)

Xavier initilization : 입,출력 갯수에 비례해서 초깃값 세팅

