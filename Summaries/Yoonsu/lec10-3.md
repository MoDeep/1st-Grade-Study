# 2018/08/26 정리 : Lec10-3
##Dropout과 앙상블

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-3-1.PNG?raw=true)
overfiting : 주어진 예제에 너무 맞춰져 있는 함수를 찾아내서 처음보는 데이터를 돌릴 때는 잘 나오지 않는 것.


![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-3-2.PNG?raw=true)

레이어를 더 깊게 쌓을 수록 오버피팅이 될 가능성이 높다.
왜냐하면 깊게 쌓일 수록 사용되는 변수가 많아 함수의 모양이 굴곡져 지기에 트레이닝 데이터에만 맞춰진 함수가 학습될 수 있기 때문이다.


오버피팅의 해결책
1. 많은 학습데이터를 갖게 한다.
2. features를 줄인다.
3. Regulatization

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-3-3.PNG?raw=true)

regulatization은 구부러진 함수의 모양을 피는 것이다.

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-3-4.PNG?raw=true)

뉴럴 네트워크에서는 Dropout를 사용한다.
이는 랜덤으로 몇개의 노드의 연결을 끊어버리는 것이다.

임의의 몇개만을 가지고 학습을 시키고 마지막에 모든 것을 가지고 학습을 시키는 것이다.
학습할 때만 임의의 노드를 제외하고 실제 데이터를 돌릴 때는 모든 노드를 가지고 학습을 시킨다.


구현
![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-3-5.PNG?raw=true)

앙상블
![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Yoonsu/images/lec10-3-6.PNG?raw=true)

각각의 Learning Model을 만들고 각각 학습을 시키면 결과가 조금씩 다르게 나온다 이 결과들을 합쳐서 마지막 결과를 도출한다.
이를 사용하면 적에는 2%에서 많게는 4%까지 성능이 향상된다.





