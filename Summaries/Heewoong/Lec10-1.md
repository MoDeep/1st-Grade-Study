## ReLU

weight와 layer를 너무 많이 넣으면, 좋은 결과가 나오지 않는 경우도   있다. 

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Heewoong/Images/lec10-1-1.PNG?raw=true) 

x가 너무 작아지기 때문에 학습에 별다른 영향을 끼치지 못하게 되는 것이다.

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Heewoong/Images/lec10-1-2.PNG?raw=true) 

경사도가 매우 낮아진다.

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Heewoong/Images/lec10-1-3.PNG?raw=true) 

sigmoid의 값이 무조건 1보다 작아지는 문제점을 보완하기 위해 만든 것이, ReLU이다. 간단한 형태인데, 0보다 크면 계속하여 증가한다.

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Heewoong/Images/lec10-1-4.PNG?raw=true) 

ReLU가 매우 잘 동작하는 것을 볼 수 있다.

![](https://github.com/MoDeep/1st-Grade-Study/blob/master/Summaries/Heewoong/Images/lec10-1-5.PNG?raw=true) 

Leaky ReLU 처럼 0이하에 0.1x를 넣는 것도 있고, Maxout이나 ELU처럼 다른 종류의 ReLU도 있다. sigmoid를 바꾼 tanh도 볼 수 있다.

